# Predicting House Prices using Linear Regression, Random Forest, and XGBoost

## 📚 Project Overview
This project aims to predict house prices using three machine learning models: Linear Regression, Random Forest, and XGBoost. The dataset used is sourced from Kaggle's "House Prices - Advanced Regression Techniques" competition.

The main objectives of the project are:
- Perform thorough data preprocessing (handling missing values, outliers, encoding, scaling).
- Train and evaluate three different regression models.
- Apply hyperparameter tuning to optimize model performance.
- Compare models based on metrics like MAE, MSE, RMSE, and R².

## 📂 Project Structure
- `Predicting_House_Prices.ipynb` — Jupyter Notebook containing the full implementation.
- `train.csv` — Training dataset.
- `test.csv` — Testing dataset.
- `data_description.txt` — Description of each feature in the dataset.
- `Predicting_House_Prices_Report.pdf` — Complete project report documenting methodology, results, and conclusions.

## 📈 Algorithms Used
- Linear Regression
- Random Forest Regressor
- XGBoost Regressor

## 🛠️ Tools & Libraries
- Python
- Jupyter Notebook (Kaggle environment)
- pandas, numpy, matplotlib, seaborn
- scikit-learn
- XGBoost

## 📊 Evaluation Metrics
- Mean Absolute Error (MAE)
- Mean Squared Error (MSE)
- Root Mean Squared Error (RMSE)
- R-squared (R²) Score

## 🔥 Key Results
- XGBoost achieved the highest accuracy among all models.
- Ensemble methods like Random Forest and XGBoost outperformed simple Linear Regression.

## 🔮 Future Scope
- Explore LightGBM, CatBoost, and Support Vector Regression (SVR).
- Implement model stacking and advanced hyperparameter tuning (Bayesian Optimization).
- Deploy the model using Streamlit or Flask for real-time predictions.

## 🏷️ Dataset Reference
- [House Prices - Advanced Regression Techniques (Kaggle)](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques)
